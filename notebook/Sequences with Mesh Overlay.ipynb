{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "literary-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import ImageFont, ImageDraw, Image, ImageColor\n",
    "import numpy as np\n",
    "import glob\n",
    "import xmltodict\n",
    "from tqdm import tqdm\n",
    "from psbody.mesh import Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "casual-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "chokepoint_root = \"/home/oole/Data/Chokepoint\"\n",
    "def _load_roidb_chokepoint(subset):\n",
    "    subset_path = os.path.join(chokepoint_root, \"annotation\", \"G1\", subset)\n",
    "    paths = sorted(glob.glob(subset_path + \"*/*/*/*\"))\n",
    "    vid_names_1 = [path for path in paths if \"xml\" not in path and \"seq\" in path]\n",
    "    vid_names_1 = [\"/\".join(v.split(\"/\")[-3:]) for v in vid_names_1]\n",
    "    paths = sorted(glob.glob(subset_path + \"*/*/*\"))\n",
    "    vid_names_2 = [path for path in paths if \"xml\" not in path and \"seq\" in path]\n",
    "    vid_names_2 = [\"/\".join(v.split(\"/\")[-2:]) for v in vid_names_2]\n",
    "    vid_names = list(vid_names_2 + vid_names_1)\n",
    "    return vid_names\n",
    "\n",
    "def get_sequences(subset):\n",
    "    subset_path = os.path.join(chokepoint_root, \"annotation\", \"G1\", subset)\n",
    "    paths = sorted(glob.glob(subset_path + \"*/*/*/*\"))\n",
    "    sequences_1 = [path for path in paths if \"xml\" not in path and \"seq\" in path]\n",
    "    sequences_1 = [\"/\".join(v.split(\"/\")[-3:]) for v in sequences_1]\n",
    "    paths = sorted(glob.glob(subset_path + \"*/*/*\"))\n",
    "    sequences_2 = [path for path in paths if \"xml\" not in path and \"seq\" in path]\n",
    "    sequences_2 = [\"/\".join(v.split(\"/\")[-2:]) for v in sequences_2]\n",
    "    sequences = list(sequences_2 + sequences_1)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adverse-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_frame_files(sequence, subset):\n",
    "    sequence_folder = os.path.join(chokepoint_root, \"annotation/G1/\", subset, sequence)\n",
    "    sequence_frame_files = sorted(glob.glob(sequence_folder + \"/*.xml\"))\n",
    "    sequence_frame_numbers = [frame_file.split(\"/\")[-1].split(\".\")[0] for frame_file in sequence_frame_files]\n",
    "    first_frame_number = sequence_frame_numbers[0]\n",
    "    return sequence_frame_files, first_frame_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions to draw image with object boundary boxes\n",
    "fontname = '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'\n",
    "font = ImageFont.truetype(fontname, 40) if os.path.isfile(fontname) else ImageFont.load_default()\n",
    "\n",
    "def bounding_box(img, xmin, ymin, xmax, ymax, width, score):\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    xres, yres = img.size[0], img.size[1]\n",
    "    box = [xmin, ymin, xmax, ymax]\n",
    "    draw.rectangle(box, outline=\"red\", width=width)\n",
    "\n",
    "def plot_img(img, axes, xmin, ymin, xmax, ymax):\n",
    "    for i in range(len(xmin)):\n",
    "        bounding_box(img, xmin[i], ymin[i], xmax[i], ymax[i], 2, -1)\n",
    "    plt.setp(axes, xticks=[], yticks=[])\n",
    "    plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "isolated-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_data_to_bbox(annotation):\n",
    "    object_ann = annotation['object']\n",
    "    bbox = object_ann['bndbox']\n",
    "    x1 = int(bbox['xmin'])\n",
    "    y1 = int(bbox['ymin'])\n",
    "    x2 = int(bbox['xmax'])\n",
    "    y2 = int(bbox['ymax'])\n",
    "    box = [x1, y1, x2, y2]\n",
    "    return box\n",
    "\n",
    "def obj_file_path(annotation):\n",
    "    sub_path = annotation['folder']\n",
    "    fname = annotation['filename']\n",
    "    full_path = os.path.join(chokepoint_root, sub_path, fname + \".jpg\")\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "applied-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ringnet preprocess:\n",
    "def resize_img(img, scale_factor):\n",
    "    new_size = (np.floor(np.array(img.shape[0:2]) * scale_factor)).astype(int)\n",
    "    new_img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "    # This is scale factor of [height, width] i.e. [y, x]\n",
    "    actual_factor = [\n",
    "        new_size[0] / float(img.shape[0]), new_size[1] / float(img.shape[1])\n",
    "    ]\n",
    "    return new_img, actual_factor\n",
    "\n",
    "def scale_and_crop(image, scale, center, img_size):\n",
    "    image_scaled, scale_factors = resize_img(image, scale)\n",
    "    # Swap so it's [x, y]\n",
    "    scale_factors = [scale_factors[1], scale_factors[0]]\n",
    "    center_scaled = np.round(center * scale_factors).astype(np.int)\n",
    "\n",
    "    margin = int(img_size / 2)\n",
    "    image_pad = np.pad(\n",
    "        image_scaled, ((margin, ), (margin, ), (0, )), mode='edge')\n",
    "    center_pad = center_scaled + margin\n",
    "    # figure out starting point\n",
    "    start_pt = center_pad - margin\n",
    "    end_pt = center_pad + margin\n",
    "    # crop:\n",
    "    crop = image_pad[start_pt[1]:end_pt[1], start_pt[0]:end_pt[0], :]\n",
    "    proc_param = {\n",
    "        'scale': scale,\n",
    "        'start_pt': start_pt,\n",
    "        'end_pt': end_pt,\n",
    "        'img_size': img_size\n",
    "    }\n",
    "\n",
    "    return crop, proc_param\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    img = io.imread(img_path)\n",
    "    if np.max(img.shape[:2]) != 224:\n",
    "        print('Resizing so the max image size is %d..' % 224)\n",
    "        scale = (float(224) / np.max(img.shape[:2]))\n",
    "    else:\n",
    "        scale = 1.0#scaling_factor\n",
    "    center = np.round(np.array(img.shape[:2]) / 2).astype(int)\n",
    "    # image center in (x,y)\n",
    "    center = center[::-1]\n",
    "    crop, proc_param = scale_and_crop(img, scale, center,\n",
    "                                               224)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # Normalize image to [-1, 1]\n",
    "    # plt.imshow(crop/255.0)\n",
    "    # plt.show()\n",
    "    crop = 2 * ((crop / 255.) - 0.5)\n",
    "\n",
    "    return crop, proc_param, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dried-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/storage/Msc/RingNet/model/ring_6_68641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/37315 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37315/37315 [05:27<00:00, 113.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create and save meshes using ringnet:\n",
    "import tensorflow._api.v2.compat.v1 as tf\n",
    "from psbody.mesh import Mesh\n",
    "from skimage import io\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "face_data_path = os.path.join(chokepoint_root, \"face_data\")\n",
    "first_batch_img_paths = sorted(glob.glob(face_data_path + \"*/*/*/*/*/*\"))\n",
    "first_batch_img_paths = [path for path in first_batch_img_paths if \".jpg\" in path]\n",
    "# only jpgs\n",
    "second_batch_img_paths = sorted(glob.glob(face_data_path + \"*/*/*/*/*/*/*\"))\n",
    "second_batch_img_paths = [path for path in second_batch_img_paths if \".jpg\" in path]\n",
    "\n",
    "all_img_paths = first_batch_img_paths + second_batch_img_paths\n",
    "assert len(all_img_paths) == len(np.unique(all_img_paths)), \"inconsistency with image paths! {} != {}\".format(len(all_img_paths), en(np.unique(all_img_paths)))\n",
    "# print(len(all_img_paths))\n",
    "# print(len(np.unique(all_img_paths)))\n",
    "\n",
    "template_mesh = Mesh(filename=\"/home/oole/git/ma/coma/impl/data/template.obj\")\n",
    "# viewer = MeshViewer()\n",
    "load_path = \"/mnt/storage/Msc/RingNet/model/ring_6_68641\"\n",
    "model_path = load_path + \".meta\"\n",
    "\n",
    "chkpoint_flame_parameters_root = \"/mnt/storage/Data/Chokepoint\"\n",
    "flame_path = os.path.join(chkpoint_flame_parameters_root, \"face_data_flame_parameters\")\n",
    "# print(\"flame_path: {}\".format(flame_path))\n",
    "\n",
    "flame_parameters_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    graph = sess.graph\n",
    "    saver = tf.compat.v1.train.import_meta_graph(model_path)\n",
    "    saver.restore(sess, load_path)\n",
    "    vertices = graph.get_tensor_by_name(u'Flamenetnormal_2/Add_9:0')\n",
    "    params = graph.get_tensor_by_name(u'add_2:0')\n",
    "    image_input = graph.get_tensor_by_name(u'input_images:0')\n",
    "    \n",
    "    i = 0\n",
    "    print(\"start processing\")\n",
    "    for img_path in tqdm(all_img_paths):\n",
    "#         sub_path_to_mesh = img_path.split(chokepoint_root)[-1].split(\"/face_data\")[-1].split(\".jpg\")[0][1:]\n",
    "#         sub_path_split = sub_path_to_mesh.split(\"/\")\n",
    "#         fname = sub_path_split[-1] + \".pickle\"\n",
    "#         sub_path = (\"/\").join(sub_path_split[:-1])\n",
    "#         full_path =os.path.join(flame_path, sub_path)\n",
    "#         print(full_path)\n",
    "#         if not os.path.exists(full_path):\n",
    "#             os.makedirs(full_path)\n",
    "#         full_path_to_flame = os.path.join(full_path, fname)\n",
    "#         full_path_fo_flame = full_path_to_flame.replace(chokepoint_root, chkpoint_flame_parameters_root)\n",
    "#         print(full_path_fo_flame)\n",
    "#         print(full_path_to_flame)\n",
    "        crop, proc_param, img = preprocess_image(img_path)\n",
    "        face_image = np.expand_dims(crop, axis=0)\n",
    "        # input_image = np.expand_dims(face_image, axis=0)\n",
    "        fetch_dict = {'vertices': vertices,\n",
    "                     'parameters': params}\n",
    "        feed_dict = {\n",
    "                image_input: face_image\n",
    "            }\n",
    "        res = sess.run(fetch_dict, feed_dict)\n",
    "        verts = res['vertices'][0]\n",
    "        parameters = res['parameters'][0]\n",
    "#         flame_parameters = res['parameters']\n",
    "#         viewer.set_dynamic_meshes([Mesh(v=verts, f=template_mesh.f)])\n",
    "#         print(full_path_to_mesh)\n",
    "        \n",
    "        flame_parameters = {\n",
    "            'img_path': img_path,\n",
    "#             'crop': crop,\n",
    "            'proc_param': proc_param,\n",
    "#             'img': img,\n",
    "            'verts': verts,\n",
    "            'parameters': parameters\n",
    "        }\n",
    "        flame_parameters_list.append(flame_parameters)\n",
    "\n",
    "        \n",
    "#         time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "illegal-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(flame_parameters_list, open(os.path.join(\"/home/oole/Data/Chokepoint/face_data_flame_parameters\", \"flame_parameters.pickle\"), 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "theoretical-costs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /mnt/storage/Msc/RingNet/model/ring_6_68641\n"
     ]
    }
   ],
   "source": [
    "# Create and save meshes using ringnet:\n",
    "import tensorflow._api.v2.compat.v1 as tf\n",
    "from psbody.mesh import Mesh\n",
    "from skimage import io\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "template_mesh = Mesh(filename=\"/home/oole/git/ma/coma/impl/data/template.obj\")\n",
    "# viewer = MeshViewer()\n",
    "load_path = \"/mnt/storage/Msc/RingNet/model/ring_6_68641\"\n",
    "model_path = load_path + \".meta\"\n",
    "\n",
    "\n",
    "my_image = \"/home/oole/Desktop/oole.jpg\"\n",
    "with tf.Session() as sess:\n",
    "    graph = sess.graph\n",
    "    saver = tf.compat.v1.train.import_meta_graph(model_path)\n",
    "    saver.restore(sess, load_path)\n",
    "    vertices = graph.get_tensor_by_name(u'Flamenetnormal_2/Add_9:0')\n",
    "    params = graph.get_tensor_by_name(u'add_2:0')\n",
    "    image_input = graph.get_tensor_by_name(u'input_images:0')\n",
    "    \n",
    "    \n",
    "    crop, proc_param, img = preprocess_image(my_image)\n",
    "    face_image = np.expand_dims(crop, axis=0)\n",
    "        \n",
    "    fetch_dict = {'vertices': vertices,\n",
    "                 'parameters': params}\n",
    "    feed_dict = {\n",
    "            image_input: face_image\n",
    "        }\n",
    "    res = sess.run(fetch_dict, feed_dict)\n",
    "    verts = res['vertices'][0]\n",
    "    parameters = res['parameters'][0]\n",
    "\n",
    "    flame_parameters = {\n",
    "        'img_path': my_image,\n",
    "#             'crop': crop,\n",
    "        'proc_param': proc_param,\n",
    "#             'img': img,\n",
    "        'verts': verts,\n",
    "        'parameters': parameters\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fifty-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(flame_parameters, open(os.path.join(\"/home/oole/Desktop/\", \"oole_flame_parameters.pickle\"), 'wb'), protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-spice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
