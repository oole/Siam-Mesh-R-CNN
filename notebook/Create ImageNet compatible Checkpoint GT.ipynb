{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "third-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "turkish-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate in the style of ILSVRC\n",
    "# <annotation>\n",
    "# \t<folder>ILSVRC2017_VID_train_0000/ILSVRC2017_train_00000000</folder>\n",
    "# \t<filename>000000</filename>\n",
    "# \t<source>\n",
    "# \t\t<database>ILSVRC_2017</database>\n",
    "# \t</source>\n",
    "# \t<size>\n",
    "# \t\t<width>1280</width>\n",
    "# \t\t<height>720</height>\n",
    "# \t</size>\n",
    "# \t<object>\n",
    "# \t\t<trackid>0</trackid>\n",
    "# \t\t<name>n01503061</name>\n",
    "# \t\t<bndbox>\n",
    "# \t\t\t<xmax>892</xmax>\n",
    "# \t\t\t<xmin>675</xmin>\n",
    "# \t\t\t<ymax>506</ymax>\n",
    "# \t\t\t<ymin>296</ymin>\n",
    "# \t\t</bndbox>\n",
    "# \t\t<occluded>0</occluded>\n",
    "# \t\t<generated>0</generated>\n",
    "# \t</object>\n",
    "# </annotation>\n",
    "import xml.etree.ElementTree as gfg\n",
    "from xml.dom import minidom\n",
    "\n",
    "def create_imagenet_style_xml(folder, \n",
    "                              filename, \n",
    "                              source, \n",
    "                              width, height, \n",
    "                              trackid, \n",
    "                              name, xmax, xmin, ymax, ymin):\n",
    "    root = gfg.Element('annotation')\n",
    "\n",
    "    folder_elem = gfg.Element(\"folder\")\n",
    "    folder_elem.text = folder\n",
    "    root.append(folder_elem)\n",
    "\n",
    "    filename_elem = gfg.Element(\"filename\")\n",
    "    filename_elem.text = filename\n",
    "    root.append(filename_elem)\n",
    "\n",
    "    source_elem = gfg.Element(\"source\")\n",
    "    source_elem.text = source\n",
    "    root.append(source_elem)\n",
    "\n",
    "    size_elem = gfg.Element(\"size\")\n",
    "    width_elem = gfg.Element(\"width\")\n",
    "    width_elem.text = width\n",
    "    size_elem.append(width_elem)\n",
    "\n",
    "    height_elem = gfg.Element(\"height\")\n",
    "    height_elem.text = height\n",
    "    size_elem.append(height_elem)\n",
    "    root.append(size_elem)\n",
    "\n",
    "    objekt_elem = gfg.Element(\"object\")\n",
    "    trackid_elem = gfg.Element(\"trackid\")\n",
    "    trackid_elem.text = trackid\n",
    "    objekt_elem.append(trackid_elem)\n",
    "\n",
    "    name_elem = gfg.Element(\"name\")\n",
    "    name_elem.text= name\n",
    "    objekt_elem.append(name_elem)\n",
    "\n",
    "    bndbox_elem = gfg.Element(\"bndbox\")\n",
    "    xmax_elem = gfg.Element(\"xmax\")\n",
    "    xmax_elem.text = xmax\n",
    "    bndbox_elem.append(xmax_elem)\n",
    "\n",
    "    xmin_elem = gfg.Element(\"xmin\")\n",
    "    xmin_elem.text = xmin\n",
    "    bndbox_elem.append(xmin_elem)\n",
    "\n",
    "    ymax_elem = gfg.Element(\"ymax\")\n",
    "    ymax_elem.text = ymax\n",
    "    bndbox_elem.append(ymax_elem)\n",
    "\n",
    "    ymin_elem = gfg.Element(\"ymin\")\n",
    "    ymin_elem.text = ymin\n",
    "    bndbox_elem.append(ymin_elem)\n",
    "    \n",
    "    objekt_elem.append(bndbox_elem)\n",
    "\n",
    "    occluded_elem = gfg.Element(\"occluded\")\n",
    "    occluded_elem.text = \"0\"\n",
    "    objekt_elem.append(occluded_elem)\n",
    "\n",
    "    generated_elem = gfg.Element(\"generated\")\n",
    "    generated_elem.text = \"0\"\n",
    "    objekt_elem.append(generated_elem)\n",
    "    \n",
    "    root.append(objekt_elem)\n",
    "\n",
    "    xml_tree = gfg.tostring(root)\n",
    "    reparsed = minidom.parseString(xml_tree)\n",
    "    #print(reparsed.childNodes[0].toprettyxml())\n",
    "    output_xml = reparsed.childNodes[0]\n",
    "    return output_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "collected-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet organisation:\n",
    "# For each video theres a train folder with annotations for each frame\n",
    "# preprocess imagenet vid:\n",
    "# takes all xmls for the given vid\n",
    "# selects two at random\n",
    "# fo rthese two it extracts the anntoation from the xml\n",
    "# then for each it extracts the object annotation from which it extracts the object.\n",
    "\n",
    "# then for each it extracts the bounding box\n",
    "# it does nothing to the data or the folder, since those are externally provided.\n",
    "# AND the filename matches the name of the annotation file, e.g. 000000.xml\n",
    "# And that is it.\n",
    "\n",
    "# So what we do:\n",
    "# For every sequence we create an annotation folder. This folder contains xml annotations,\n",
    "# where the name corresponds to the file name of the frame that it describes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "labeled-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageSet for training: train.txt\n",
    "# Create an ImageSet for testing: test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "directed-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on G2 Eval sequences\n",
    "# Test on G1 Eval Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "younger-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G1\n",
    "g1_path = \"/home/oole/git/projects-to-consume/chokepoint-bbs/G1\"\n",
    "g1_all_sequence_files = glob.glob(os.path.join(g1_path, \"*.txt\"))\n",
    "g1_all_sequence_files.sort()\n",
    "g1_train_sequence_file = [file for file in g1_all_sequence_files if \"train\" in file][0]\n",
    "g1_eval_sequences_files = [file for file in g1_all_sequence_files if \"eval\" in file]\n",
    "if len(g1_train_sequence_file) is None or len(g1_eval_sequences_files) < 10:\n",
    "    print(\"G2 sequences?\")\n",
    "\n",
    "g1_eval_dict = {}\n",
    "for eval_sequence in g1_eval_sequences_files:\n",
    "    eval_scenes, eval_frame_names, eval_xmins, eval_ymins, eval_xmaxs, eval_ymaxs = [], [], [] ,[] ,[], []\n",
    "    with open(eval_sequence) as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            file_path, ymin, xmin, width, height  = line.split(',')\n",
    "            _, scene, frame_name = file_path.split(\"/\")\n",
    "            eval_scenes.append(scene)\n",
    "            eval_frame_names.append(frame_name)\n",
    "            eval_xmins.append(int(xmin))\n",
    "            eval_ymins.append(int(ymin))\n",
    "            eval_xmaxs.append(int(xmin)+int(width))\n",
    "            eval_ymaxs.append(int(ymin)+int(height))\n",
    "    if not len(eval_scenes) == len(eval_frame_names) == len(eval_xmins) == len(eval_ymins) == len(eval_xmaxs) == len(eval_ymaxs):\n",
    "        print('probem')\n",
    "        break\n",
    "    g1_eval_dict[eval_sequence] = {\n",
    "        'scenes': eval_scenes, \n",
    "        'frame_names': eval_frame_names,\n",
    "        'xmins': eval_xmins,\n",
    "        'ymins': eval_ymins,\n",
    "        'xmaxs': eval_xmaxs,\n",
    "        'ymaxs': eval_ymaxs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "useful-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G2\n",
    "g2_path = \"/home/oole/git/projects-to-consume/chokepoint-bbs/G2\"\n",
    "g2_all_sequence_files = glob.glob(os.path.join(g2_path, \"*.txt\"))\n",
    "g2_all_sequence_files.sort()\n",
    "g2_train_sequence_file = [file for file in g2_all_sequence_files if \"train\" in file][0]\n",
    "g2_eval_sequences_files = [file for file in g2_all_sequence_files if \"eval\" in file]\n",
    "if len(g2_train_sequence_file) is None or len(g2_eval_sequences_files) < 10:\n",
    "    print(\"G2 sequences?\")\n",
    "    \n",
    "g2_eval_dict = {}\n",
    "for eval_sequence in g2_eval_sequences_files:\n",
    "    eval_scenes, eval_frame_names, eval_xmins, eval_ymins, eval_xmaxs, eval_ymaxs = [], [], [] ,[] ,[], []\n",
    "    with open(eval_sequence) as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            file_path, ymin, xmin, width, height  = line.split(',')\n",
    "            _, scene, frame_name = file_path.split(\"/\")\n",
    "            eval_scenes.append(scene)\n",
    "            eval_frame_names.append(frame_name)\n",
    "            eval_xmins.append(int(xmin))\n",
    "            eval_ymins.append(int(ymin))\n",
    "            eval_xmaxs.append(int(xmin)+int(width))\n",
    "            eval_ymaxs.append(int(ymin)+int(height))\n",
    "    if not len(eval_scenes) == len(eval_frame_names) == len(eval_xmins) == len(eval_ymins) == len(eval_xmaxs) == len(eval_ymaxs):\n",
    "        print('probem')\n",
    "        break\n",
    "    g2_eval_dict[eval_sequence] = {\n",
    "        'scenes': eval_scenes, \n",
    "        'frame_names': eval_frame_names,\n",
    "        'xmins': eval_xmins,\n",
    "        'ymins': eval_ymins,\n",
    "        'xmaxs': eval_xmaxs,\n",
    "        'ymaxs': eval_ymaxs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "liable-polymer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g2_eval_sequences_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "published-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(xmin,ymin,xmax, ymax):\n",
    "    expand_by =10\n",
    "    xmin = xmin-expand_by\n",
    "    xmax = xmax+expand_by\n",
    "    ymin = ymin-expand_by\n",
    "    ymax = ymax+expand_by\n",
    "    return xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "rubber-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/oole/Data/Chokepoint\"\n",
    "\n",
    "annotation_path = os.path.join(base_path,\"annotation\")\n",
    "\n",
    "relative_path_to_sequences = \"data/\"\n",
    "\n",
    "\n",
    "def write_eval_dict_to_xml(eval_dict, group, mode, key_list=None):\n",
    "    count_scene_dict = {}\n",
    "#     found_one_with_point = False\n",
    "#     printed_one_without_point = False\n",
    "    if key_list is None:\n",
    "        dict_keys =eval_dict.keys()\n",
    "    else:\n",
    "        dict_keys = key_list\n",
    "        print(\"using supplied key list\")\n",
    "    for key in dict_keys:\n",
    "        sequence = eval_dict[key]\n",
    "        unique_scenes = np.unique(sequence['scenes'])\n",
    "        assert len(unique_scenes) == 1\n",
    "        sub_scene_dir = unique_scenes[0]\n",
    "        scene_split = sub_scene_dir.split('_')\n",
    "        if '.' in sub_scene_dir:\n",
    "#             print(sub_scene_dir)\n",
    "            three_dir_layers = True\n",
    "#             found_one_with_point = True\n",
    "            in_between_scene_dir = scene_split[0] + '_' + scene_split[1] + \"_\" + scene_split[2].split('.')[0]\n",
    "            top_scene_dir = scene_split[0] + '_' + scene_split[1]\n",
    "#             print(\"top: {}\".format(top_scene_dir))\n",
    "#             print(\"in_between_scene_dir: {}\".format(in_between_scene_dir))\n",
    "#             print(\"sub: {}\".format(sub_scene_dir))\n",
    "        else:\n",
    "            top_scene_dir = scene_split[0] + '_' + scene_split[1]\n",
    "            three_dir_layers = False\n",
    "#             if not printed_one_without_point:\n",
    "#                 print(\"normal top: {}\".format(top_scene_dir))\n",
    "#                 print(\"sub_scene_dir: {}\".format(sub_scene_dir))\n",
    "#                 printed_one_without_point = True\n",
    "            \n",
    "#         print(top_scene_dir)\n",
    "        frames = sequence['frame_names']\n",
    "        xmins = sequence['xmins']\n",
    "        ymins = sequence['ymins']\n",
    "        xmaxs = sequence['xmaxs']\n",
    "        ymaxs = sequence['ymaxs']\n",
    "        key_split = key.split('_')\n",
    "        sequence_name = key_split[2] + '_' + key_split[3].split('.')[0]\n",
    "        sequence_dir = sub_scene_dir + \"_\" + sequence_name\n",
    "        if three_dir_layers:\n",
    "            sequence_dir_path = os.path.join(annotation_path, group, mode, \n",
    "                                             top_scene_dir, in_between_scene_dir,sequence_dir)\n",
    "        else:\n",
    "            sequence_dir_path = os.path.join(annotation_path, group, mode, top_scene_dir, sequence_dir)\n",
    "        if not os.path.exists(sequence_dir_path):\n",
    "            os.makedirs(sequence_dir_path)\n",
    "#         print(sequence_dir_path)\n",
    "        for idx, frame in enumerate(frames):\n",
    "#             print(frame)\n",
    "            frame_number = frame.split('.')[0]\n",
    "#             print(frame_number)\n",
    "            xmin = xmins[idx]\n",
    "            ymin = ymins[idx]\n",
    "            xmax = xmaxs[idx]\n",
    "            ymax = ymaxs[idx]\n",
    "            xmin,ymin,xmax, ymax =  expand_bbox(xmin,ymin,xmax, ymax)\n",
    "            if three_dir_layers:\n",
    "                folder = os.path.join(relative_path_to_sequences, top_scene_dir, in_between_scene_dir, sub_scene_dir)\n",
    "            else:\n",
    "                folder = os.path.join(relative_path_to_sequences, top_scene_dir, sub_scene_dir)\n",
    "            filename = frame_number\n",
    "            source = \"Chokepoint_Dataset\"\n",
    "            width = \"800\"\n",
    "            height = \"600\"\n",
    "            trackid = \"0\"\n",
    "            name = \"face\"\n",
    "            xml = create_imagenet_style_xml(folder,\n",
    "                                      filename, \n",
    "                                      source, \n",
    "                                      width, \n",
    "                                      height, \n",
    "                                      trackid, \n",
    "                                      name, \n",
    "                                      str(xmax), \n",
    "                                      str(xmin), \n",
    "                                      str(ymax), \n",
    "                                      str(ymin))\n",
    "            xml_file_path = os.path.join(sequence_dir_path, frame_number + \".xml\")\n",
    "#             break\n",
    "            with open(xml_file_path, \"w\") as file:\n",
    "                file.write(xml.toprettyxml())\n",
    "            \n",
    "            \n",
    "#             print(xml_file_path)\n",
    "#             print(xml.toprettyxml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "polished-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_eval_dict_to_xml(g2_eval_dict, group = \"G1\", mode = \"train\")\n",
    "# \t<folder>data/P2L_S2_C2.2/P2L_S2_C2.2</folder>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "polar-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2_keys = g2_eval_dict.keys()\n",
    "# print(len(keys))\n",
    "# seq_numbers = []\n",
    "# for key in keys:\n",
    "#     seq_number = key.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "#     print(seq_number)\n",
    "# per scene there are 25 sequences\n",
    "# There are \n",
    "g2_num_val =  int(len(g2_keys) * 0.2)\n",
    "g2_num_val\n",
    "import random\n",
    "random.seed(1)\n",
    "g2_keys_list = list(g2_keys)\n",
    "random.shuffle(g2_keys_list)\n",
    "# for key in keys_list:\n",
    "#     print(key)\n",
    "g2_val_keys = g2_keys_list[:g2_num_val]\n",
    "g2_train_keys = g2_keys_list[g2_num_val:]\n",
    "len(g2_val_keys)\n",
    "for key in g2_val_keys:\n",
    "    if key in g2_train_keys:\n",
    "        print(\"alarm\")\n",
    "for key in g2_train_keys:\n",
    "    if key in g2_val_keys:\n",
    "        print(\"alarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "mounted-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in g1_eval_dict.keys():\n",
    "#     elem = g1_eval_dict[key]\n",
    "#     print(elem)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "historical-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using supplied key list\n",
      "using supplied key list\n",
      "using supplied key list\n"
     ]
    }
   ],
   "source": [
    "# G1:\n",
    "# use sequence from g2_eval dict (which is the train partition) for validation\n",
    "# \n",
    "write_eval_dict_to_xml(g2_eval_dict, group = \"G1\", mode = \"train\", key_list=g2_train_keys)\n",
    "write_eval_dict_to_xml(g2_eval_dict, group = \"G1\", mode = \"val\", key_list=g2_val_keys)\n",
    "write_eval_dict_to_xml(g1_eval_dict, group = \"G1\", mode = \"test\", key_list=g1_eval_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "strong-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_keys = g1_eval_dict.keys()\n",
    "# print(len(keys))\n",
    "# seq_numbers = []\n",
    "# for key in keys:\n",
    "#     seq_number = key.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "#     print(seq_number)\n",
    "# per scene there are 25 sequences\n",
    "# There are \n",
    "g1_num_val =  int(len(g1_keys) * 0.2)\n",
    "g1_num_val\n",
    "import random\n",
    "random.seed(1)\n",
    "g1_keys_list = list(g1_keys)\n",
    "random.shuffle(g1_keys_list)\n",
    "# for key in keys_list:\n",
    "#     print(key)\n",
    "g1_val_keys = g1_keys_list[:g1_num_val]\n",
    "g1_train_keys = g1_keys_list[g1_num_val:]\n",
    "len(g1_val_keys)\n",
    "for key in g1_val_keys:\n",
    "    if key in g1_train_keys:\n",
    "        print(\"alarm\")\n",
    "for key in g1_train_keys:\n",
    "    if key in g1_val_keys:\n",
    "        print(\"alarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "cardiovascular-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using supplied key list\n",
      "using supplied key list\n",
      "using supplied key list\n"
     ]
    }
   ],
   "source": [
    "# G2:\n",
    "write_eval_dict_to_xml(g1_eval_dict, group = \"G2\", mode = \"train\", key_list=g1_train_keys)\n",
    "write_eval_dict_to_xml(g1_eval_dict, group = \"G2\", mode = \"val\", key_list=g1_val_keys)\n",
    "write_eval_dict_to_xml(g2_eval_dict, group = \"G2\", mode = \"test\", key_list=g2_eval_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "exotic-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_roidb_chokepoint(subset)...\n",
    "CHOKEPOINT_ROOT = \"/home/oole/Data/Chokepoint\"\n",
    "subset = \"train\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "connected-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oole/Data/Chokepoint/annotation/G1/train/P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_159\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "paths = sorted(glob.glob(subset_path + \"*/*/*/*\"))\n",
    "vid_names_1 = [path for path in paths if \"xml\" not in path and \"seq\" in path]\n",
    "for vid_name in vid_names_1:\n",
    "    if \"P2L_S1_C1.1_seq_159\" in vid_name:\n",
    "        print(vid_name)\n",
    "vid_names_1 = [\"/\".join(v.split(\"/\")[-3:]) for v in vid_names_1]\n",
    "                \n",
    "paths = sorted(glob.glob(subset_path + \"*/*/*\"))\n",
    "vid_names_2 = [path for path in paths if \"xml\" not in path and \"seq\" in path]\n",
    "for vid_name in vid_names_2:\n",
    "    if \"P2L_S1_C1.1_seq_159\" in vid_name:\n",
    "        print(vid_name)\n",
    "vid_names_2 = [\"/\".join(v.split(\"/\")[-2:]) for v in vid_names_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "continent-debut",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P1E_S1/P1E_S1_C1_seq_1',\n",
       " 'P1E_S1/P1E_S1_C1_seq_10',\n",
       " 'P1E_S1/P1E_S1_C1_seq_11',\n",
       " 'P1E_S1/P1E_S1_C1_seq_12',\n",
       " 'P1E_S1/P1E_S1_C1_seq_13',\n",
       " 'P1E_S1/P1E_S1_C1_seq_14',\n",
       " 'P1E_S1/P1E_S1_C1_seq_15',\n",
       " 'P1E_S1/P1E_S1_C1_seq_16',\n",
       " 'P1E_S1/P1E_S1_C1_seq_17',\n",
       " 'P1E_S1/P1E_S1_C1_seq_18',\n",
       " 'P1E_S1/P1E_S1_C1_seq_19',\n",
       " 'P1E_S1/P1E_S1_C1_seq_2',\n",
       " 'P1E_S1/P1E_S1_C1_seq_20',\n",
       " 'P1E_S1/P1E_S1_C1_seq_21',\n",
       " 'P1E_S1/P1E_S1_C1_seq_22',\n",
       " 'P1E_S1/P1E_S1_C1_seq_23',\n",
       " 'P1E_S1/P1E_S1_C1_seq_24',\n",
       " 'P1E_S1/P1E_S1_C1_seq_25',\n",
       " 'P1E_S1/P1E_S1_C1_seq_3',\n",
       " 'P1E_S1/P1E_S1_C1_seq_4',\n",
       " 'P1E_S1/P1E_S1_C1_seq_5',\n",
       " 'P1E_S1/P1E_S1_C1_seq_6',\n",
       " 'P1E_S1/P1E_S1_C1_seq_7',\n",
       " 'P1E_S1/P1E_S1_C1_seq_8',\n",
       " 'P1E_S1/P1E_S1_C1_seq_9',\n",
       " 'P1E_S2/P1E_S2_C2_seq_26',\n",
       " 'P1E_S2/P1E_S2_C2_seq_27',\n",
       " 'P1E_S2/P1E_S2_C2_seq_28',\n",
       " 'P1E_S2/P1E_S2_C2_seq_29',\n",
       " 'P1E_S2/P1E_S2_C2_seq_30',\n",
       " 'P1E_S2/P1E_S2_C2_seq_31',\n",
       " 'P1E_S2/P1E_S2_C2_seq_32',\n",
       " 'P1E_S2/P1E_S2_C2_seq_33',\n",
       " 'P1E_S2/P1E_S2_C2_seq_34',\n",
       " 'P1E_S2/P1E_S2_C2_seq_35',\n",
       " 'P1E_S2/P1E_S2_C2_seq_36',\n",
       " 'P1E_S2/P1E_S2_C2_seq_37',\n",
       " 'P1E_S2/P1E_S2_C2_seq_38',\n",
       " 'P1E_S2/P1E_S2_C2_seq_39',\n",
       " 'P1E_S2/P1E_S2_C2_seq_40',\n",
       " 'P1E_S2/P1E_S2_C2_seq_41',\n",
       " 'P1E_S2/P1E_S2_C2_seq_42',\n",
       " 'P1E_S2/P1E_S2_C2_seq_43',\n",
       " 'P1E_S2/P1E_S2_C2_seq_44',\n",
       " 'P1E_S2/P1E_S2_C2_seq_45',\n",
       " 'P1E_S2/P1E_S2_C2_seq_46',\n",
       " 'P1E_S2/P1E_S2_C2_seq_47',\n",
       " 'P1E_S2/P1E_S2_C2_seq_48',\n",
       " 'P1E_S2/P1E_S2_C2_seq_49',\n",
       " 'P1E_S2/P1E_S2_C2_seq_50',\n",
       " 'P1L_S1/P1L_S1_C1_seq_51',\n",
       " 'P1L_S1/P1L_S1_C1_seq_52',\n",
       " 'P1L_S1/P1L_S1_C1_seq_53',\n",
       " 'P1L_S1/P1L_S1_C1_seq_54',\n",
       " 'P1L_S1/P1L_S1_C1_seq_55',\n",
       " 'P1L_S1/P1L_S1_C1_seq_56',\n",
       " 'P1L_S1/P1L_S1_C1_seq_57',\n",
       " 'P1L_S1/P1L_S1_C1_seq_58',\n",
       " 'P1L_S1/P1L_S1_C1_seq_59',\n",
       " 'P1L_S1/P1L_S1_C1_seq_60',\n",
       " 'P1L_S1/P1L_S1_C1_seq_61',\n",
       " 'P1L_S1/P1L_S1_C1_seq_62',\n",
       " 'P1L_S1/P1L_S1_C1_seq_63',\n",
       " 'P1L_S1/P1L_S1_C1_seq_64',\n",
       " 'P1L_S1/P1L_S1_C1_seq_65',\n",
       " 'P1L_S1/P1L_S1_C1_seq_66',\n",
       " 'P1L_S1/P1L_S1_C1_seq_67',\n",
       " 'P1L_S1/P1L_S1_C1_seq_68',\n",
       " 'P1L_S1/P1L_S1_C1_seq_69',\n",
       " 'P1L_S1/P1L_S1_C1_seq_70',\n",
       " 'P1L_S1/P1L_S1_C1_seq_71',\n",
       " 'P1L_S1/P1L_S1_C1_seq_72',\n",
       " 'P1L_S1/P1L_S1_C1_seq_73',\n",
       " 'P1L_S1/P1L_S1_C1_seq_74',\n",
       " 'P1L_S1/P1L_S1_C1_seq_75',\n",
       " 'P1L_S2/P1L_S2_C2_seq_100',\n",
       " 'P1L_S2/P1L_S2_C2_seq_76',\n",
       " 'P1L_S2/P1L_S2_C2_seq_77',\n",
       " 'P1L_S2/P1L_S2_C2_seq_78',\n",
       " 'P1L_S2/P1L_S2_C2_seq_79',\n",
       " 'P1L_S2/P1L_S2_C2_seq_80',\n",
       " 'P1L_S2/P1L_S2_C2_seq_81',\n",
       " 'P1L_S2/P1L_S2_C2_seq_82',\n",
       " 'P1L_S2/P1L_S2_C2_seq_83',\n",
       " 'P1L_S2/P1L_S2_C2_seq_84',\n",
       " 'P1L_S2/P1L_S2_C2_seq_85',\n",
       " 'P1L_S2/P1L_S2_C2_seq_86',\n",
       " 'P1L_S2/P1L_S2_C2_seq_87',\n",
       " 'P1L_S2/P1L_S2_C2_seq_88',\n",
       " 'P1L_S2/P1L_S2_C2_seq_89',\n",
       " 'P1L_S2/P1L_S2_C2_seq_90',\n",
       " 'P1L_S2/P1L_S2_C2_seq_91',\n",
       " 'P1L_S2/P1L_S2_C2_seq_92',\n",
       " 'P1L_S2/P1L_S2_C2_seq_93',\n",
       " 'P1L_S2/P1L_S2_C2_seq_94',\n",
       " 'P1L_S2/P1L_S2_C2_seq_95',\n",
       " 'P1L_S2/P1L_S2_C2_seq_96',\n",
       " 'P1L_S2/P1L_S2_C2_seq_97',\n",
       " 'P1L_S2/P1L_S2_C2_seq_98',\n",
       " 'P1L_S2/P1L_S2_C2_seq_99',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_101',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_102',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_103',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_104',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_105',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_106',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_107',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_108',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_109',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_110',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_111',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_112',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_113',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_114',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_115',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_116',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_117',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_118',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_119',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_120',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_121',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_122',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_123',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.1_seq_124',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.2_seq_125',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.2_seq_126',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.2_seq_127',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.2_seq_128',\n",
       " 'P2E_S1/P2E_S1_C3/P2E_S1_C3.2_seq_129',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_130',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_131',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_132',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_133',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_134',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_135',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_136',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_137',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_138',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_139',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_140',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_141',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_142',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_143',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_144',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_145',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_146',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_147',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_148',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_149',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_150',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_151',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_152',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.1_seq_153',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.2_seq_154',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.2_seq_155',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.2_seq_156',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.2_seq_157',\n",
       " 'P2E_S2/P2E_S2_C2/P2E_S2_C2.2_seq_158',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_159',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_160',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_161',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_162',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_163',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_164',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_165',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_166',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_167',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_168',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_169',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_170',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_171',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_172',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_173',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_174',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_175',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_176',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_177',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_178',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_179',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_180',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_181',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_182',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.1_seq_183',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.2_seq_184',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.2_seq_185',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.2_seq_186',\n",
       " 'P2L_S1/P2L_S1_C1/P2L_S1_C1.2_seq_187',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_188',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_189',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_190',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_191',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_192',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_193',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_194',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_195',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_196',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_197',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_198',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_199',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_200',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_201',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_202',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_203',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_204',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_205',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_206',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_207',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_208',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_209',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_210',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_211',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.1_seq_212',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.2_seq_213',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.2_seq_214',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.2_seq_215',\n",
       " 'P2L_S2/P2L_S2_C2/P2L_S2_C2.2_seq_216']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_names = list(vid_names_2 + vid_names_1)\n",
    "vid_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-provider",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
