{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "statistical-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "drawn-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_g1 = \"/home/oole/git/projects-to-consume/chokepoint-bbs/G1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "alike-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequence_files = glob.glob(os.path.join(path_to_g1, \"*.txt\"))\n",
    "all_sequence_files.sort()\n",
    "#all_sequence_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "sharp-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence_file = [file for file in all_sequence_files if \"train\" in file][0]\n",
    "eval_sequences_files = [file for file in all_sequence_files if \"eval\" in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "falling-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes, frame_names, xmins, ymins, xmaxs, ymaxs = [], [], [] ,[] ,[], []\n",
    "with open(train_sequence_file) as file:\n",
    "    for line in file:\n",
    "        line = line.strip('\\n')\n",
    "        file_path, ymin, xmin, width, height  = line.split(',')\n",
    "        _, scene, frame_name = file_path.split(\"/\")\n",
    "        scenes.append(scene)\n",
    "        frame_names.append(frame_name)\n",
    "        xmins.append(xmin)\n",
    "        ymins.append(ymin)\n",
    "        xmaxs.append(xmin+width)\n",
    "        ymaxs.append(ymin+height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "premier-permit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scenes) == len(frame_names) == len(xmins) == len(ymins) == len(xmaxs) == len(ymaxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "supported-fellowship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16665"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "advisory-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_scenes = np.unique(scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "pacific-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {}\n",
    "for eval_sequence in eval_sequences_files:\n",
    "    eval_scenes, eval_frame_names, eval_xmins, eval_ymins, eval_xmaxs, eval_ymaxs = [], [], [] ,[] ,[], []\n",
    "    with open(eval_sequence) as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            file_path, ymin, xmin, width, height  = line.split(',')\n",
    "            _, scene, frame_name = file_path.split(\"/\")\n",
    "            eval_scenes.append(scene)\n",
    "            eval_frame_names.append(frame_name)\n",
    "            eval_xmins.append(xmin)\n",
    "            eval_ymins.append(ymin)\n",
    "            eval_xmaxs.append(xmin+width)\n",
    "            eval_ymaxs.append(ymin+height)\n",
    "    if not len(eval_scenes) == len(eval_frame_names) == len(eval_xmins) == len(eval_ymins) == len(eval_xmaxs) == len(eval_ymaxs):\n",
    "        print('probem')\n",
    "        break\n",
    "    eval_dict[eval_sequence] = {\n",
    "        'scenes': eval_scenes, \n",
    "        'frame_names': eval_frame_names,\n",
    "        'x_mins': eval_xmins,\n",
    "        'ymins': eval_ymins,\n",
    "        'xmaxs': eval_xmaxs,\n",
    "        'ymaxs': eval_ymaxs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "urban-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "south-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scenes = []\n",
    "for key in eval_dict.keys():\n",
    "    eval_scenes.extend(eval_dict[key]['scenes'])\n",
    "unique_eval_scenes = np.unique(eval_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "gentle-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene in unique_train_scenes:\n",
    "    if scene in unique_eval_scenes:\n",
    "        print(\"ohoh: {}\".format(scene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "biological-namibia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scenes:\n",
      " ['P1E_S1_C1' 'P1E_S2_C2' 'P1L_S1_C1' 'P1L_S2_C2' 'P2E_S1_C3.1'\n",
      " 'P2E_S1_C3.2' 'P2E_S2_C2.1' 'P2E_S2_C2.2' 'P2L_S1_C1.1' 'P2L_S1_C1.2'\n",
      " 'P2L_S2_C2.1' 'P2L_S2_C2.2']\n",
      "Eval scenes:\n",
      " ['P1E_S3_C3' 'P1E_S4_C1' 'P1L_S3_C3' 'P1L_S4_C1' 'P2E_S3_C1.1'\n",
      " 'P2E_S3_C1.2' 'P2E_S4_C2.1' 'P2E_S4_C2.2' 'P2L_S3_C3.1' 'P2L_S3_C3.2'\n",
      " 'P2L_S4_C2.1' 'P2L_S4_C2.2']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train scenes:\\n {}\".format(unique_train_scenes))\n",
    "print(\"Eval scenes:\\n {}\".format(unique_eval_scenes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aboriginal-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_g2 = \"/home/oole/git/projects-to-consume/chokepoint-bbs/G2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "impressive-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequence_files_g2 = glob.glob(os.path.join(path_to_g2, \"*.txt\"))\n",
    "all_sequence_files_g2.sort()\n",
    "#all_sequence_files\n",
    "train_sequence_file_g2 = [file for file in all_sequence_files_g2 if \"train\" in file][0]\n",
    "eval_sequences_files_g2 = [file for file in all_sequence_files_g2 if \"eval\" in file]\n",
    "scenes_g2, frame_names_g2, xmins_g2, ymins_g2, xmaxs_g2, ymaxs_g2 = [], [], [] ,[] ,[], []\n",
    "with open(train_sequence_file_g2) as file:\n",
    "    for line in file:\n",
    "        line = line.strip('\\n')\n",
    "        file_path, ymin, xmin, width, height  = line.split(',')\n",
    "        _, scene, frame_name = file_path.split(\"/\")\n",
    "        scenes_g2.append(scene)\n",
    "        frame_names_g2.append(frame_name)\n",
    "        xmins_g2.append(xmin)\n",
    "        ymins_g2.append(ymin)\n",
    "        xmaxs_g2.append(xmin+width)\n",
    "        ymaxs_g2.append(ymin+height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "turned-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict_g2 = {}\n",
    "for eval_sequence in eval_sequences_files_g2:\n",
    "    eval_scenes, eval_frame_names, eval_xmins, eval_ymins, eval_xmaxs, eval_ymaxs = [], [], [] ,[] ,[], []\n",
    "    with open(eval_sequence) as file:\n",
    "        for line in file:\n",
    "            line = line.strip('\\n')\n",
    "            file_path, ymin, xmin, width, height  = line.split(',')\n",
    "            _, scene, frame_name = file_path.split(\"/\")\n",
    "            eval_scenes.append(scene)\n",
    "            eval_frame_names.append(frame_name)\n",
    "            eval_xmins.append(xmin)\n",
    "            eval_ymins.append(ymin)\n",
    "            eval_xmaxs.append(xmin+width)\n",
    "            eval_ymaxs.append(ymin+height)\n",
    "    if not len(eval_scenes) == len(eval_frame_names) == len(eval_xmins) == len(eval_ymins) == len(eval_xmaxs) == len(eval_ymaxs):\n",
    "        print('probem')\n",
    "        break\n",
    "    eval_dict_g2[eval_sequence] = {\n",
    "        'scenes': eval_scenes, \n",
    "        'frame_names': eval_frame_names,\n",
    "        'x_mins': eval_xmins,\n",
    "        'ymins': eval_ymins,\n",
    "        'xmaxs': eval_xmaxs,\n",
    "        'ymaxs': eval_ymaxs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "controlled-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scenes_g2 = []\n",
    "for key in eval_dict_g2.keys():\n",
    "    eval_scenes_g2.extend(eval_dict_g2[key]['scenes'])\n",
    "unique_eval_scenes_g2 = np.unique(eval_scenes_g2)\n",
    "unique_train_scenes_g2 = np.unique(scenes_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "related-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene in unique_train_scenes_g2:\n",
    "    if scene in unique_eval_scenes_g2:\n",
    "        print(\"ohoh: {}\".format(scene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "stylish-brazilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scenes G1:\n",
      " ['P1E_S1_C1' 'P1E_S2_C2' 'P1L_S1_C1' 'P1L_S2_C2' 'P2E_S1_C3.1'\n",
      " 'P2E_S1_C3.2' 'P2E_S2_C2.1' 'P2E_S2_C2.2' 'P2L_S1_C1.1' 'P2L_S1_C1.2'\n",
      " 'P2L_S2_C2.1' 'P2L_S2_C2.2']\n",
      "Train scenes G2:\n",
      " ['P1E_S3_C3' 'P1E_S4_C1' 'P1L_S3_C3' 'P1L_S4_C1' 'P2E_S3_C1.1'\n",
      " 'P2E_S3_C1.2' 'P2E_S4_C2.1' 'P2E_S4_C2.2' 'P2L_S3_C3.1' 'P2L_S3_C3.2'\n",
      " 'P2L_S4_C2.1' 'P2L_S4_C2.2']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train scenes G1:\\n {}\".format(unique_train_scenes))\n",
    "print(\"Train scenes G2:\\n {}\".format(unique_train_scenes_g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "suffering-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval scenes G1:\n",
      " ['P1E_S3_C3' 'P1E_S4_C1' 'P1L_S3_C3' 'P1L_S4_C1' 'P2E_S3_C1.1'\n",
      " 'P2E_S3_C1.2' 'P2E_S4_C2.1' 'P2E_S4_C2.2' 'P2L_S3_C3.1' 'P2L_S3_C3.2'\n",
      " 'P2L_S4_C2.1' 'P2L_S4_C2.2']\n",
      "Eval scenes G2:\n",
      " ['P1E_S1_C1' 'P1E_S2_C2' 'P1L_S1_C1' 'P1L_S2_C2' 'P2E_S1_C3.1'\n",
      " 'P2E_S1_C3.2' 'P2E_S2_C2.1' 'P2E_S2_C2.2' 'P2L_S1_C1.1' 'P2L_S1_C1.2'\n",
      " 'P2L_S2_C2.1' 'P2L_S2_C2.2']\n"
     ]
    }
   ],
   "source": [
    "print(\"Eval scenes G1:\\n {}\".format(unique_eval_scenes))\n",
    "print(\"Eval scenes G2:\\n {}\".format(unique_eval_scenes_g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "descending-rings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(unique_train_scenes) == set(unique_eval_scenes_g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "executive-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each eval annotation file contains exactly one sequence with one annotated face accross the sequence.\n",
    "# The train sequence is a concatenation of all training sequence of the other group.\n",
    "# So for our case the eval sequences of G2 can be used as train sequences for G1, and then\n",
    "# evaluate on the evaluation sequences of G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "hungry-repository",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1 total train frames: 16665\n",
      "G2 total eval frames: 16664\n"
     ]
    }
   ],
   "source": [
    "# sanity check on the size of the squences\n",
    "total_eval_frames_g2 = 0\n",
    "for key in eval_dict_g2.keys():\n",
    "    total_eval_frames_g2 += len(eval_dict_g2[key]['frame_names'])\n",
    "print(\"G1 total train frames: {}\".format(len(frame_names)))\n",
    "print(\"G2 total eval frames: {}\".format(total_eval_frames_g2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cardiac-london",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G2 total train frames: 20652\n",
      "G1 total eval frames: 20651\n"
     ]
    }
   ],
   "source": [
    "# sanity check on the size of the squences\n",
    "total_eval_frames_g1 = 0\n",
    "for key in eval_dict.keys():\n",
    "    total_eval_frames_g1 += len(eval_dict[key]['frame_names'])\n",
    "print(\"G2 total train frames: {}\".format(len(frame_names_g2)))\n",
    "print(\"G1 total eval frames: {}\".format(total_eval_frames_g1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "continental-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to write out these sequences nicely, so that they can be read by the dataset/dataflow\n",
    "# for training and evaluating the model.\n",
    "path_to_chockepoint_base_folder = \"/home/oole/Data/Chokepoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "historic-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\")):\n",
    "    os.mkdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\"))\n",
    "    os.mkdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\", \"g1\"))\n",
    "    os.mkdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\", \"g1\",\"train\"))\n",
    "    os.mkdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\", \"g1\",\"test\"))\n",
    "    os.mkdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\", \"g2\"))\n",
    "    os.mkdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\", \"g2\", \"train\"))\n",
    "    os.mkdir(os.path.join(path_to_chockepoint_base_folder, \"annotation\", \"g2\", \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "southeast-cotton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation>\n",
      "\t<folder>ILSVRC2017_VID_train_0000/ILSVRC2017_train_00000000</folder>\n",
      "\t<filename>000000</filename>\n",
      "\t<source>ILSVRC_2017</source>\n",
      "\t<object>\n",
      "\t\t<trackid>0</trackid>\n",
      "\t\t<name>n01503061</name>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmax>892</xmax>\n",
      "\t\t\t<xmin>675</xmin>\n",
      "\t\t\t<ymax>506</ymax>\n",
      "\t\t\t<ymin>296</ymin>\n",
      "\t\t</bndbox>\n",
      "\t\t<occluded>0</occluded>\n",
      "\t\t<generated>0</generated>\n",
      "\t</object>\n",
      "</annotation>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Annotate in the style of ILSVRC\n",
    "# <annotation>\n",
    "# \t<folder>ILSVRC2017_VID_train_0000/ILSVRC2017_train_00000000</folder>\n",
    "# \t<filename>000000</filename>\n",
    "# \t<source>\n",
    "# \t\t<database>ILSVRC_2017</database>\n",
    "# \t</source>\n",
    "# \t<size>\n",
    "# \t\t<width>1280</width>\n",
    "# \t\t<height>720</height>\n",
    "# \t</size>\n",
    "# \t<object>\n",
    "# \t\t<trackid>0</trackid>\n",
    "# \t\t<name>n01503061</name>\n",
    "# \t\t<bndbox>\n",
    "# \t\t\t<xmax>892</xmax>\n",
    "# \t\t\t<xmin>675</xmin>\n",
    "# \t\t\t<ymax>506</ymax>\n",
    "# \t\t\t<ymin>296</ymin>\n",
    "# \t\t</bndbox>\n",
    "# \t\t<occluded>0</occluded>\n",
    "# \t\t<generated>0</generated>\n",
    "# \t</object>\n",
    "# </annotation>\n",
    "import xml.etree.ElementTree as gfg\n",
    "from xml.dom import minidom\n",
    "root = gfg.Element('annotation')\n",
    "\n",
    "folder = gfg.Element(\"folder\")\n",
    "folder.text = \"ILSVRC2017_VID_train_0000/ILSVRC2017_train_00000000\"\n",
    "root.append(folder)\n",
    "\n",
    "filename = gfg.Element(\"filename\")\n",
    "filename.text = \"000000\"\n",
    "root.append(filename)\n",
    "\n",
    "source = gfg.Element(\"source\")\n",
    "source.text = \"ILSVRC_2017\"\n",
    "root.append(source)\n",
    "\n",
    "size = gfg.Element(\"size\")\n",
    "width = gfg.Element(\"width\")\n",
    "width.text = \"1280\"\n",
    "size.append(width)\n",
    "\n",
    "height = gfg.Element(\"height\")\n",
    "height.text = \"720\"\n",
    "size.append(height)\n",
    "\n",
    "objekt = gfg.Element(\"object\")\n",
    "trackid = gfg.Element(\"trackid\")\n",
    "trackid.text = \"0\"\n",
    "objekt.append(trackid)\n",
    "\n",
    "name = gfg.Element(\"name\")\n",
    "name.text=\"n01503061\"\n",
    "objekt.append(name)\n",
    "\n",
    "bndbox = gfg.Element(\"bndbox\")\n",
    "xmax = gfg.Element(\"xmax\")\n",
    "xmax.text = \"892\"\n",
    "bndbox.append(xmax)\n",
    "\n",
    "xmin = gfg.Element(\"xmin\")\n",
    "xmin.text = \"675\"\n",
    "bndbox.append(xmin)\n",
    "\n",
    "ymax = gfg.Element(\"ymax\")\n",
    "ymax.text = \"506\"\n",
    "bndbox.append(ymax)\n",
    "\n",
    "ymin = gfg.Element(\"ymin\")\n",
    "ymin.text = \"296\"\n",
    "bndbox.append(ymin)\n",
    "objekt.append(bndbox)\n",
    "\n",
    "occluded = gfg.Element(\"occluded\")\n",
    "occluded.text = \"0\"\n",
    "objekt.append(occluded)\n",
    "\n",
    "root.append(objekt)\n",
    "\n",
    "generated = gfg.Element(\"generated\")\n",
    "generated.text = \"0\"\n",
    "objekt.append(generated)\n",
    "\n",
    "xml_tree = gfg.tostring(root)\n",
    "reparsed = minidom.parseString(xml_tree)\n",
    "print(reparsed.childNodes[0].toprettyxml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "julian-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet organisation:\n",
    "# For each video theres a train folder with annotations for each frame\n",
    "# preprocess imagenet vid:\n",
    "# takes all xmls for the given vid\n",
    "# selects two at random\n",
    "# fo rthese two it extracts the anntoation from the xml\n",
    "# then for each it extracts the object annotation from which it extracts the object.\n",
    "\n",
    "# then for each it extracts the bounding box\n",
    "# it does nothing to the data or the folder, since those are externally provided.\n",
    "# AND the filename matches the name of the annotation file, e.g. 000000.xml\n",
    "# And that is it.\n",
    "\n",
    "# So what we do:\n",
    "# For every sequence we create an annotation folder. This folder contains xml annotations,\n",
    "# where the name corresponds to the file name of the frame that it describes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-hormone",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
